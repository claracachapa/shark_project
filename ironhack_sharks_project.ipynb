{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3949f1-70c1-4b79-80ed-f02b7c763434",
   "metadata": {},
   "source": [
    "# Shark Attack Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8758dc-0f14-4867-a76d-a7d39b05fd07",
   "metadata": {},
   "source": [
    "### Created by Alvaro, Aurelie, Clara and Marc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7584f-70f4-46fc-abb1-6743e082620f",
   "metadata": {},
   "source": [
    "For our project, we assessed and processed the data based on our business proposal of a \"shark repellent\" (Ã  la snake oil). We are looking mostly at who we should advertise to and in which countries those advertisements should run, thus only cleaning and processing for the columns that concern our marketing/ads strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82862817-b3ea-4227-aa43-5ea09dff5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re #for handling the date column\n",
    "\n",
    "#for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "url = 'https://www.sharkattackfile.net/spreadsheets/GSAF5.xls'\n",
    "shark_df = pd.read_excel(url)\n",
    "\n",
    "# Working with a copy in order not to overwrite the original dataset with code with errors\n",
    "shark_df_copy = shark_df.copy()\n",
    "shark_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad02f4f-3b08-48ae-8d37-e4c192176e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_df_copy.info()\n",
    "#We can see that some columns are hidden in the Excel file and are unnecessary for our research, therefore we should drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa25f0-6567-4521-881a-9571cd2d5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns from 15 to 22\n",
    "columns_to_drop = shark_df_copy.columns[15:23]\n",
    "shark_df_copy = shark_df_copy.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "#Standardizing the names of every column \n",
    "shark_df_copy.columns = shark_df_copy.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "#Dropping every row that has the value 'Invalid' in its type to prevent working with useless data\n",
    "shark_df_copy.drop(shark_df_copy[shark_df_copy['type'] == 'Invalid'].index, inplace = True)\n",
    "shark_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a0b30-c95a-4119-a83c-b53189bdc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all of the unnecessary columns for our research\n",
    "shark_df_copy = shark_df_copy.drop(columns=['state', 'location', 'name', 'source', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731805b7-5a2e-4e73-8df9-bc6a182f3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_season(shark_df_copy):\n",
    "    #date format\n",
    "    shark_df_copy['date'] = pd.to_datetime(shark_df_copy['date'], errors='coerce').dt.date\n",
    "    shark_df_copy.dropna(subset=['date'], inplace=True) # drop a few use case I can't format\n",
    "    shark_df_copy.drop_duplicates(subset=['date'], inplace=True)\n",
    "    shark_df_copy.sort_values(by='date', inplace=True) # not sure if we keep this line it just sort by date\n",
    "    \n",
    "    \n",
    "    # function to differentiate the hemispheres and convert dates to seasons\n",
    "    def determine_hemisphere(country):\n",
    "        southern_hemisphere = ['australia', 'brazil', 'south africa', 'argentina', \"seychelles\"]  # to determine later\n",
    "        return 'south' if country in southern_hemisphere else 'north'\n",
    "    #create a hemisphere column just for test purpose\n",
    "    shark_df_copy['hemisphere'] = shark_df_copy['country'].apply(determine_hemisphere)\n",
    "    \n",
    "    def get_season(date, hemisphere):\n",
    "        month = date.month\n",
    "        if hemisphere == 'north':\n",
    "            if month in [12, 1, 2]:\n",
    "                return 'Winter'\n",
    "            elif month in [3, 4, 5]:\n",
    "                return 'Spring'\n",
    "            elif month in [6, 7, 8]:\n",
    "                return 'Summer'\n",
    "            else:\n",
    "                return 'Fall'\n",
    "        else:  # South\n",
    "            if month in [12, 1, 2]:\n",
    "                return 'Summer'\n",
    "            elif month in [3, 4, 5]:\n",
    "                return 'Fall'\n",
    "            elif month in [6, 7, 8]:\n",
    "                return 'Winter'\n",
    "            else:\n",
    "                return 'Spring'\n",
    "    \n",
    "    shark_df_copy['season'] = shark_df_copy.apply(lambda row: get_season(row['date'], row['hemisphere']), axis=1)\n",
    "    display(shark_df_copy)\n",
    "clean_season(shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23bb41-73e9-4b0a-b06a-332542e4b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for times values and part of the day\n",
    "def clean_time_and_time_of_day(shark_df_copy):\n",
    "    def clean_time(time_str):\n",
    "        if pd.isna(time_str):\n",
    "            return None #fill with mean() \n",
    "        # Remove non-numeric and non-h characters\n",
    "        time_str = re.sub(r'[^0-9h]', '', time_str)\n",
    "    \n",
    "        # If the time format is hMM or hhMM\n",
    "        match = re.match(r'(\\d{1,2})h(\\d{2})$', time_str)\n",
    "        if match:\n",
    "            return f\"{match.group(1).zfill(2)}:{match.group(2)}\"\n",
    "    \n",
    "        # If the time format is hhh or hhMM\n",
    "        match = re.match(r'(\\d{1,3})h$', time_str)\n",
    "        if match:\n",
    "            return pd.to_datetime(match.group(1).zfill(4), format='%H%M', errors='coerce').strftime('%H:%M')\n",
    "    \n",
    "        # Default case to handle any other remaining formats\n",
    "        match = re.match(r'(\\d{1,2})h(\\d{2})?$', time_str)\n",
    "        if match:\n",
    "            hour = match.group(1).zfill(2)\n",
    "            minute = match.group(2) if match.group(2) else '00'\n",
    "            return f\"{hour}:{minute}\"\n",
    "        return None #fill with mean() \n",
    "    \n",
    "\n",
    "    \n",
    "    def time_of_day(hour):\n",
    "        if 5 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 17:\n",
    "            return 'Afternoon'\n",
    "        elif 17 <= hour < 21:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "    \n",
    "    \n",
    "    # Ensure 'time' column is properly formatted as string\n",
    "    shark_df_copy['time'] = shark_df_copy['time'].astype(str)\n",
    "    shark_df_copy['time'] = shark_df_copy['time'].apply(clean_time)\n",
    "    \n",
    "    valid_times = pd.to_datetime(shark_df_copy['time'], format='%H:%M', errors='coerce')\n",
    "    \n",
    "    mean_time = valid_times.dropna().mean()\n",
    "    mean_time_str = mean_time.strftime('%H:%M')\n",
    "    shark_df_copy['time'] = shark_df_copy['time'].fillna(mean_time_str)\n",
    "    \n",
    "    shark_df_copy['time'] = pd.to_datetime(shark_df_copy['time'], format='%H:%M', errors='coerce').dt.time\n",
    "    \n",
    "    shark_df_copy['time_of_day'] = shark_df_copy['time'].apply(lambda x: time_of_day(x.hour) if pd.notnull(x) else 'Invalid Time')\n",
    "    display(shark_df_copy)\n",
    "\n",
    "clean_time_and_time_of_day(shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af8400-094d-4875-bae3-ca0544528130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the sex column and replacing the typos for the intended gender\n",
    "def clean_sex(shark_df_copy):\n",
    "  shark_df_copy[\"sex\"] = shark_df_copy[\"sex\"].str.strip()\n",
    "  shark_df_copy[\"sex\"] = shark_df_copy[\"sex\"].replace(\"M x 2\", \"M\")\n",
    "  shark_df_copy[\"sex\"] = shark_df_copy[\"sex\"].replace(\"N\", \"M\")\n",
    "  shark_df_copy[\"sex\"] = shark_df_copy[\"sex\"].replace(\"lli\", \"M\")\n",
    "  shark_df_copy[\"sex\"] = shark_df_copy[\"sex\"].replace(\".\", \"M\")\n",
    "\n",
    "  value_counts = shark_df_copy['sex'].value_counts(dropna=False)\n",
    "  display(value_counts)\n",
    "\n",
    "clean_sex(shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d561b1-bb1e-4656-8997-9ab12f366700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the country column and dropping all rows with null values in it.\n",
    "def clean_country(shark_df_copy):\n",
    "    shark_df_copy.dropna(subset=[\"country\"], inplace=True) \n",
    "    shark_df_copy[\"country\"] = shark_df_copy['country'].str.lower().str.replace(' ', '_') \n",
    "    primary_countries = shark_df_copy[\"country\"].value_counts() #show the primary countries where shark attacks occur\n",
    "    total_attacks = shark_df_copy['country'].count()\n",
    "    percentage_by_country = (primary_countries/total_attacks)*100\n",
    "\n",
    "    display(primary_countries.head(20))\n",
    "    print('\\n')\n",
    "    display(percentage_by_country)\n",
    "clean_country(shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e27278-c8f3-40cd-907b-b95cb1b634c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the activity column by creating a priority map allows us to change the initial values to \"categories\" and make the data more understandable.\n",
    "#In this map, order matters and we use it to our advantage.\n",
    "def clean_activity(shark_df_copy):\n",
    "  priority_map = [\n",
    "      (['disaster', 'adrift', 'sunk', 'sink', 'wreck', 'founde', 'sank', 'capsiz', 'overboard'], 'Disaster'), #this is the first priority because we do not want to overwrite \"overboard\" as \"Surfing/ Boarding\"\n",
    "      (['surf', 'board', 'sruf', 'paddl', 'sup'], 'Surfing/ Boarding'),\n",
    "      (['swim', 'float', 'swm'], 'Swimming/ Floating'),\n",
    "      (['fish', 'lobster', 'scallop', 'shrimp', 'crab', 'oyster', 'hunt', 'harpoon', 'fihi'], 'Fishing'),\n",
    "      (['boat', 'kayak', 'ship', 'raft', 'canoe', 'row', 'sail', 'yacht', 'jet ski'], 'Boating/ Watercraft'),\n",
    "      (['wad'], 'Wading'),\n",
    "      (['bath'], 'Bathing'),\n",
    "      (['diving', 'dive'], 'Diving'),\n",
    "      (['snork'], 'Snorkeling'),\n",
    "      (['stand', 'sit', 'squat'], 'Standing/ Sitting'),\n",
    "      (['shark'], 'Handling/ Looking at Shark'),\n",
    "      (['play'], 'Playing')\n",
    "  ]\n",
    "\n",
    "#\n",
    "  def categorize_activity_priority(activity):\n",
    "      if pd.isna(activity): #for missing values\n",
    "          return \"Unknown\"\n",
    "      for keywords, category in priority_map:\n",
    "          if any(keyword in activity.lower() for keyword in keywords):\n",
    "              return category\n",
    "      return \"Other\"\n",
    "\n",
    "#looking at the different categories and how many values of each there are to compare with the uncleaned version\n",
    "  shark_df_copy['activity'] = shark_df_copy['activity'].apply(categorize_activity_priority)\n",
    "  print(shark_df_copy['activity'].value_counts())\n",
    "  return shark_df_copy\n",
    "\n",
    "clean_activity(shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a3e20-a0a2-490c-8e35-54abb2a64381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the unnamed col, renaming it and change any values that might be typos\n",
    "def clean_fatal(shark_df_copy):\n",
    "    # Get the column name at index 7\n",
    "    col_name = shark_df_copy.columns[7]\n",
    "    \n",
    "    # Rename the column\n",
    "    shark_df_copy.rename(columns={col_name: 'was_it_fatal'}, inplace=True)\n",
    "    \n",
    "    # Ensure the column was renamed correctly\n",
    "    if 'was_it_fatal' in shark_df_copy.columns:\n",
    "        shark_df_copy['was_it_fatal'] = shark_df_copy['was_it_fatal'].str.lower().str.strip()\n",
    "\n",
    "    # Checking the current values\n",
    "    print(shark_df_copy['was_it_fatal'].value_counts())\n",
    "    print(shark_df_copy['was_it_fatal'].isna().sum()) \n",
    "\n",
    "    # Creating a map that changes the typos for their intended value\n",
    "    fatality_map = {\n",
    "    'y': 'Yes',\n",
    "    'f': 'Yes',\n",
    "    'y x 2': 'Yes',\n",
    "    'n': 'No',\n",
    "    'm': 'No',\n",
    "    'nq': 'No',\n",
    "    'unknown': 'Unknown',\n",
    "    }\n",
    "\n",
    "    print('\\n')\n",
    "    shark_df_copy['was_it_fatal'] = shark_df_copy['was_it_fatal'].map(fatality_map).fillna('Unknown')\n",
    "    print(shark_df_copy['was_it_fatal'].value_counts()) \n",
    "clean_fatal(shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0b634-ad79-43f1-baf3-b202915262e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and categorizing the ages\n",
    "def clean_age(shark_df_copy):\n",
    "    # Calculating the frequency of each age and filter for values with freq > 1\n",
    "    frequency = shark_df_copy[\"age\"].value_counts()\n",
    "    values_to_keep = frequency[frequency > 1].index\n",
    "    \n",
    "    # Dropping every null value and values with frequency = 1\n",
    "    shark_df_copy = shark_df_copy[shark_df_copy['age'].isin(values_to_keep)]\n",
    "    \n",
    "    # Replacing \"N/A\" and non-numeric values with NaN\n",
    "    shark_df_copy.loc[:, 'age'] = pd.to_numeric(shark_df_copy['age'], errors='coerce')\n",
    "    shark_df_copy = shark_df_copy.dropna(subset=['age'])\n",
    "    \n",
    "    # Converting age to integers now that we only have numeric values\n",
    "    shark_df_copy.loc[:, 'age'] = shark_df_copy['age'].astype(int)\n",
    "    \n",
    "    # Defining age bins and labels\n",
    "    bins = [0, 12, 20, 30, 45, 64, float('inf')]\n",
    "    labels = ['Children', 'Teenagers', 'Young Adults', 'Middle-aged Adults', 'Older Adults', 'Seniors']\n",
    "    \n",
    "    # Applying pd.cut to categorize ages\n",
    "    shark_df_copy.loc[:, 'age_category'] = pd.cut(shark_df_copy['age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    return shark_df_copy\n",
    "\n",
    "# Updating the dataframe to have the new column\n",
    "shark_df_copy = clean_age(shark_df_copy)\n",
    "\n",
    "clean_age(shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558e94d-7c7d-4713-bb7e-53b35e5c1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating frequencies and percentages of attacks by age category\n",
    "freq = shark_df_copy[\"age_category\"].value_counts()\n",
    "total_attacks = freq.sum()\n",
    "percentage = freq / total_attacks * 100\n",
    "\n",
    "# Creating a DataFrame with the Frequency and Percentage columns\n",
    "summary_shark_df_copy = pd.DataFrame({'Frequency': freq, 'Percentage': percentage})\n",
    "\n",
    "display(summary_shark_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e1447-6c4f-43ba-9bc3-ad03bfc7424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for attacks per age category and sex\n",
    "pivot_table = shark_df_copy.pivot_table(index='age_category', columns='sex', aggfunc='size', fill_value=0)\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7894ffa-b454-4a97-baa6-7eecccb7b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activities visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "activity_counts = shark_df_copy['activity'].value_counts()\n",
    "sns.barplot(x=activity_counts.index, y=activity_counts.values , palette='hls')\n",
    "plt.title('Shark Attacks by Activity Type', fontsize=15)\n",
    "plt.xlabel('Activity', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b9932-fbcd-475d-a7d8-faafb00d250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatalities pie\n",
    "plt.figure(figsize=(10, 7))\n",
    "fatal_counts = shark_df_copy['was_it_fatal'].value_counts()\n",
    "plt.pie(fatal_counts, labels=fatal_counts.index, autopct='%1.1f%%',\n",
    "        colors=['indianred', 'orange', 'lightgray'])\n",
    "plt.title('Shark Attack Fatality Rates', fontsize=12)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88af600-2af5-41de-aedc-40bdd853cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attacks by time of the day\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "time_counts = shark_df_copy['time_of_day'].value_counts()\n",
    "sns.barplot(x=time_counts.index, y=time_counts.values,\n",
    "            order=['Morning', 'Afternoon', 'Evening', 'Night'], palette='rocket')\n",
    "plt.title('Shark Attacks by Time of Day', fontsize=15)\n",
    "plt.xlabel('Time of Day', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd48b89-aeb1-4437-a769-a3938b8cd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shark attacks by age category\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "age_counts = shark_df_copy['age_category'].value_counts()\n",
    "sns.barplot(x=age_counts.index, y=age_counts.values, palette='rocket')\n",
    "plt.title('Shark Attacks by Age Category', fontsize=15)\n",
    "plt.xlabel('Age Category', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab696c-9bde-47b8-a5d9-12d146bd019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shark attacks by season\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "time_counts = shark_df_copy['season'].value_counts()\n",
    "sns.barplot(x=time_counts.index, y=time_counts.values,\n",
    "            order=['Summer', 'Fall', 'Winter', 'Spring'], palette='husl')\n",
    "plt.title('Shark Attacks by Season', fontsize=15)\n",
    "plt.xlabel('Time of Day', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae9cd4-cece-4633-928e-d9f542702669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shark attacks by country\n",
    "\n",
    "countries_counts = shark_df_copy['country'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=countries_counts.index, y=countries_counts.values, palette='husl')\n",
    "plt.title('Shark Attacks by Country', fontsize=15)\n",
    "plt.xlabel('Countries', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0cf4d-c834-4b73-8a23-6f960e4f6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shark attacks by gender\n",
    "\n",
    "gender_counts = shark_df_copy['sex'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=gender_counts.index, y=gender_counts.values, palette='husl')\n",
    "plt.title('Shark Attacks by Gender', fontsize=15)\n",
    "plt.xlabel('Gender', fontsize=12)\n",
    "plt.ylabel('Number of Attacks', fontsize=12)\n",
    "#plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
